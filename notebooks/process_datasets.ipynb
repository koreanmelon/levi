{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.transforms import Compose, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = pathlib.Path(\"../datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations(path: pathlib.Path) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"Reads dataset annotations from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (pathlib.Path): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[str], list[int]]: A tuple containing the labels and bounding boxes.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    labels: list[str] = df[\"class\"].tolist()\n",
    "    boxes: list[int] = df[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values.tolist()\n",
    "    return labels, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_class_to_int(labels: list[str], mapping: dict[int, str]) -> list[int]:\n",
    "    \"\"\"Maps class labels to integer values.\n",
    "\n",
    "    Args:\n",
    "        labels (list[str]): List of class labels.\n",
    "        mapping (dict[int, str]): A dictionary mapping class labels to integer values.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of integer values.\n",
    "    \"\"\"\n",
    "    keys = list(mapping.keys())\n",
    "    vals = list(mapping.values())\n",
    "\n",
    "    return [keys[vals.index(label)] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_num in range(1, 6):\n",
    "    dataset_dir = datasets_dir / f\"dataset_{dataset_num:02}\"\n",
    "    images_dir = dataset_dir / \"images\"\n",
    "    annotations_csv = dataset_dir / \"annotations.csv\"\n",
    "    annotations_pt = dataset_dir / \"annotations.pt\"\n",
    "\n",
    "    labels, boxes = read_annotations(annotations_csv)\n",
    "\n",
    "    targets = {\n",
    "        \"labels\": labels,\n",
    "        \"boxes\": boxes\n",
    "    }\n",
    "\n",
    "    files = [file for file in images_dir.iterdir() if file.is_file()\n",
    "             and file.suffix == \".jpg\"]\n",
    "\n",
    "    torch.save(targets, annotations_pt)\n",
    "    data = torch.load(annotations_pt)\n",
    "    assert targets == data, f\"Targets and loaded data are not equal for dataset {dataset_num:02}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Column(Enum):\n",
    "    FILENAME = 0\n",
    "    WIDTH = 1\n",
    "    HEIGHT = 2\n",
    "    CLASS = 3\n",
    "    XMIN = 4\n",
    "    YMIN = 5\n",
    "    XMAX = 6\n",
    "    YMAX = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrbTrackingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_file: pathlib.Path,\n",
    "        img_dir: pathlib.Path,\n",
    "        mapping: dict[int, str],\n",
    "        transform=None\n",
    "    ) -> None:\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Any:\n",
    "        filename = str(self.annotations.iloc[idx, Column.FILENAME.value])\n",
    "        img_path = os.path.join(\n",
    "            self.img_dir, filename\n",
    "        )\n",
    "        image = read_image(img_path)\n",
    "        label = str(self.annotations.iloc[idx, Column.CLASS.value])\n",
    "        classname = label\n",
    "\n",
    "        labels = map_class_to_int([label], self.mapping)\n",
    "        labels = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "        boxes = self.annotations.iloc[idx,\n",
    "                                      Column.XMIN.value:Column.YMAX.value+1].values.tolist()\n",
    "        boxes = torch.tensor([boxes], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\n",
    "            \"labels\": labels,\n",
    "            \"boxes\": boxes\n",
    "        }\n",
    "\n",
    "        target = {key: value.numpy() for key, value in target.items()}\n",
    "\n",
    "        return {\n",
    "            \"x\": image,\n",
    "            \"y\": target,\n",
    "            \"x_name\": img_path,\n",
    "            \"y_name\": classname\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    0: \"background\",\n",
    "    1: \"orb\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OrbTrackingDataset(\n",
    "    annotations_file=datasets_dir / \"dataset_01\" / \"annotations.csv\",\n",
    "    img_dir=datasets_dir / \"dataset_01\" / \"images\",\n",
    "    mapping=mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"x\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_double(batch):\n",
    "    \"\"\"collate function for the ObjectDetectionDataSet.\n",
    "    Only used by the dataloader.\n",
    "\n",
    "    Credit: https://johschmidt42.medium.com/train-your-own-object-detector-with-faster-rcnn-pytorch-8d3c759cfc70\n",
    "    \"\"\"\n",
    "    x = [sample['x'] for sample in batch]\n",
    "    y = [sample['y'] for sample in batch]\n",
    "    x_name = [sample['x_name'] for sample in batch]\n",
    "    y_name = [sample['y_name'] for sample in batch]\n",
    "    return x, y, x_name, y_name\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_double\n",
    ")\n",
    "\n",
    "transform = GeneralizedRCNNTransform(min_size=1280,\n",
    "                                     max_size=1280,\n",
    "                                     image_mean=[0.485, 0.456, 0.406],\n",
    "                                     image_std=[0.229, 0.224, 0.225])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
